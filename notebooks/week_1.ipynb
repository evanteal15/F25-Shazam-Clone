{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c9acb8",
   "metadata": {},
   "source": [
    "# Intro to Digital Audio\n",
    "\n",
    "To be able to work with audio signals in a computer, we need to create a series of discrete values from a continuous sound wave.\n",
    "\n",
    "This notebook will walk you through the fundamentals of how audio is represented in Python, how to create a custom dataset of music using yt-dlp/musicdl, and how to visualize the waveform of an audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92e900",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MichiganDataScienceTeam/F25-Shazam/blob/main/notebooks/week1_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574769f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib librosa scipy yt-dlp pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ffeaa",
   "metadata": {},
   "source": [
    "\n",
    "## What is a sound wave?\n",
    "\n",
    "A sound wave is created by the vibration of air molecules.\n",
    "\n",
    "\n",
    "- **frequency**: number of times that air particles vibrate back and forth per second (Hz). \n",
    "    - This is perceived by us as the pitch\n",
    "- **amplitude**: maximum distance that air particles are displaced (from rest) as the sound wave passes\n",
    "    - This is perceived by us as the loudness, measured in decibels (dB)\n",
    "\n",
    "We perceive frequency and amplitude **logarithmically**; as frequency or amplitude increases, it takes more of a change in the respective quantity to produce the same percieved change in pitch/loudness.\n",
    "\n",
    "![metadata](./asset/sample_metadata.png)\n",
    "\n",
    "Let's break down some common aspects of an audio file:\n",
    "\n",
    "- **sample rate** - number of samples per second (Hz). The sampling rate determines the time resolution of our representation.\n",
    "    - Time is digitized using the sample rate\n",
    "    - The time in seconds between consecutive samples is $1/\\mathrm{sr}$, called the sampling interval\n",
    "- **bit depth** - The number of bits used to store each sample. The larger the bit depth, the more precision we have to store amplitude information (amplitude resolution).\n",
    "    - Amplitude is digitized using bit depth\n",
    "    \n",
    "<br>\n",
    "\n",
    "When working with MP3 files, another useful piece of information is the bit rate - the data transfer rate expressed as bits per second.\n",
    "- $\\mathrm{bit\\ depth} \\neq \\mathrm{bit\\ rate}$\n",
    "- $\\mathrm{bit\\ rate} = \\mathrm{sr}\\times\\mathrm{bit\\ depth}\\times\\textrm{number\\ of\\ channels}$\n",
    "- mp3 files lose bit depth information during the lossy compression process\n",
    "\n",
    "## Digital Waveform Representation\n",
    "\n",
    "![waveform](./asset/digital_waveform.jpg)\n",
    "\n",
    "Modify the code below to try out different values of `freq_Hz` and `sample_rate_Hz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ce5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of the audio signal\n",
    "freq_Hz = 5\n",
    "\n",
    "# rate at which the audio signal is being sampled\n",
    "sample_rate_Hz = 10\n",
    "\n",
    "x_source = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_source = np.cos(freq_Hz * 2*np.pi*x_source)\n",
    "\n",
    "x_samples = np.linspace(0, 1, sample_rate_Hz, endpoint=False)\n",
    "y_samples = np.cos(freq_Hz * 2*np.pi*x_samples)\n",
    "\n",
    "plt.plot(x_source, y_source)\n",
    "plt.scatter(x_samples, y_samples, color=\"red\", s=50)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3c2bb",
   "metadata": {},
   "source": [
    "## How many samples do we need?\n",
    "\n",
    "The samples inside a wav file are used to reconstruct the original audio as a continuous waveform to be played back for listening.\n",
    "\n",
    "**Nyquist-Shannon sampling theorem**: If a signal contains no frequencies higher than $f_\\mathrm{max}$, then the signal can be perfectly reconstructed when sampled at a rate $sr > 2f_\\mathrm{max}$. In other words, the maximum reconstructable frequency is strictly less than $sr/2$.\n",
    "\n",
    "When there is a frequency higher than $sr/2$ in the signal, the frequency instead appears in the reconstructed signal at a lower frequency than the original. This effect is called **aliasing**, and is usually undesirable.\n",
    "\n",
    "**Question**: In order to cover the human hearing range of 20 Hz to 20 kHz, what is the minimum sampling rate required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_rate_required = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_Hz = 5\n",
    "freq_Hz = 7\n",
    "\n",
    "# The Nyquist frequency is defined as sr/2\n",
    "# Any sinusoid with freq > sr/2 has an alias with freq < sr/2\n",
    "#     alias: sinusoid indistinguisable by sampling alone (same samples)\n",
    "k: int = 1  # any integer\n",
    "freq_alias_wave_Hz = np.abs(freq_Hz - k*sample_rate_Hz)\n",
    "\n",
    "x_source = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_source = np.cos(freq_Hz * 2*np.pi*x_source)\n",
    "\n",
    "x_reconstructed = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_reconstructed = np.cos(min(freq_alias_wave_Hz, freq_Hz) * 2*np.pi*x_reconstructed)\n",
    "\n",
    "x_samples = np.linspace(0, 1, sample_rate_Hz, endpoint=False)\n",
    "y_samples = np.cos(freq_Hz * 2*np.pi*x_samples)\n",
    "\n",
    "plt.plot(x_source, y_source)\n",
    "plt.plot(x_reconstructed, y_reconstructed, color=\"red\", linestyle=\"--\")\n",
    "plt.scatter(x_samples, y_samples, color=\"red\", s=50)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61deb37b",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "To create a database of songs that our Shazam clone will be able to recognize, we can use the [yt-dlp](https://github.com/yt-dlp/yt-dlp) Python package to download audio files directly from YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97125266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "# add any youtube video url here, \n",
    "# copied from browser address bar\n",
    "youtube_url = \"\"\n",
    "\n",
    "yt_audio_path = \"yt_sample.wav\"\n",
    "\n",
    "ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': \"yt_sample.%(ext)s\",  # output file\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',     # save as wav file\n",
    "        }],\n",
    "        #'cookiefile': 'cookies.txt',\n",
    "    }\n",
    "if youtube_url != \"\":\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34d356",
   "metadata": {},
   "source": [
    "## musicdl: helper script for downloading audio\n",
    "\n",
    "For convienience, we've created a yt-dlp wrapper program that accepts urls from either YouTube or Spotify, and downloads the respective audio files to a specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/dennisfarmer/musicdl.git\n",
    "%pip install -e ./musicdl\n",
    "from musicdl.yt import YoutubeDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b074ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(YoutubeDownloader.download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "ydl = YoutubeDownloader(\n",
    "    audio_directory=\"./tracks\", \n",
    "    audio_format=\"wav\"\n",
    ")\n",
    "\n",
    "youtube_urls = [\n",
    "    \"https://www.youtube.com/watch?v=TqxfdNm4gZQ\"\n",
    "]\n",
    "\n",
    "tracks_info = ydl.download(youtube_urls)\n",
    "pprint(tracks_info)\n",
    "\n",
    "# write to csv\n",
    "tracks_csv = \"./tracks/tracks_info.csv\"\n",
    "tracks_df = pd.DataFrame(tracks_info)\n",
    "tracks_df.to_csv(tracks_csv, index=False)\n",
    "\n",
    "# read from csv\n",
    "#with open(tracks_csv, \"r\") as f:\n",
    "    #tracks_df = pd.read_csv(f)\n",
    "    #tracks_list = list(tracks_df.to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0884c",
   "metadata": {},
   "source": [
    "## Visualizing the amplitude and sample rate of an audio file\n",
    "\n",
    "[`librosa.load()` documentation](https://librosa.org/doc/0.11.0/generated/librosa.load.html#librosa.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac27996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "#audio_path = yt_audio_path\n",
    "audio_path = \"sample.wav\"\n",
    "ipd.Audio(audio_path)\n",
    "\n",
    "# Use librosa.load() to read file at audio_path\n",
    "# default is to convert to mono (1 channel)\n",
    "# audio, sr = ???\n",
    "#print(f\"sample rate = {sr} Hz\")\n",
    "\n",
    "#plt.figure(figsize=(12, 4))\n",
    "#plt.plot(audio, lw=0.1)  # interpolated waveform\n",
    "#plt.scatter(np.arange(len(audio)), audio, s=0.005, color=\"red\")  # individual samples\n",
    "#plt.title(\"Audio Waveform\")\n",
    "#plt.xlabel(\"Sample Index\")\n",
    "#plt.ylabel(\"Amplitude\")\n",
    "#plt.show()\n",
    "\n",
    "#print(\"audio is digitally represented as a numpy.ndarray:\")\n",
    "#audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e8a67",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "\n",
    "internally, downsampling (reducing the sample rate) performs the following process:\n",
    "1. uses a low pass anti-aliasing filter to remove higher frequency components\n",
    "    - low pass: keep signals with frequency lower than a specified cutoff frequency\n",
    "2. keeps every nth sample using a sample step size of $(\\mathrm{sr_{old}}/\\mathrm{sr_{new}})$\n",
    "    - the details of this process are not super important to know\n",
    "    - if $(\\mathrm{sr_{old}}/\\mathrm{sr_{new}})$ is not an integer, first upsamples via interpolation, then downsamples with step size $\\mathrm{sr_{old}}$.\n",
    "    - upsampling: each original sample is separated by inserting $(\\mathrm{sr_{new}}-1)$ zeros, then signal is interpolated with filter to smooth out discontinuities (replacing zeros)\n",
    "\n",
    "**Question**: In the low pass filter step, what would the cutoff frequency be if we resampled to 11,025 Hz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff_frequency = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample from 48,000 Hz to 11,025 Hz\n",
    "# use librosa.load()\n",
    "sr = 11_025\n",
    "\n",
    "# Question: how would you plot just the first 10 seconds of an audio file?\n",
    "# Hint: use sample rate\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653caec9",
   "metadata": {},
   "source": [
    "# Task for today:\n",
    "\n",
    "1) download the audio from any youtube video as a .wav file\n",
    "2) downsample the file to 44.1 kHz\n",
    "3) crop the audio to any 5 second segment\n",
    "4) visualize the waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c37e75",
   "metadata": {},
   "source": [
    "---\n",
    "# Next week: visualizing frequency using a spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "audio_path = \"log_scale_perception.wav\"\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee19bbe",
   "metadata": {},
   "source": [
    "## Method 1: Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "\n",
    "# parameters of the short-time Fourier transform:\n",
    "# (algorithm that creates the spectrogram)\n",
    "win_length = 2**11  # number of samples in each window\n",
    "n_fft = win_length\n",
    "hop_length = win_length // 4\n",
    "window = scipy.signal.get_window(\"triang\", Nx=win_length)\n",
    "\n",
    "S = librosa.stft(audio, \n",
    "                       n_fft=n_fft, hop_length=hop_length, \n",
    "                       win_length=win_length, window=window)\n",
    "S_magnitude = np.abs(S)  # |a+bi| = sqrt(a^2 + b^2)\n",
    "S_db = librosa.amplitude_to_db(S_magnitude, ref=np.max)\n",
    "\n",
    "im = plt.imshow(S_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bfa7c4",
   "metadata": {},
   "source": [
    "## Method 2: Scipy (what we'll use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07492ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "\n",
    "# parameters of the short-time Fourier transform:\n",
    "# (algorithm that creates the spectrogram)\n",
    "nperseg = win_length = 2**11  # number of samples in each window\n",
    "nfft = n_fft = win_length\n",
    "hop_length = win_length // 4\n",
    "window = scipy.signal.get_window(\"triang\", Nx=win_length)\n",
    "\n",
    "# scipy.signal.stft also uses the sample rate to output \n",
    "# frequency (in Hz) and time (in seconds) vectors,\n",
    "# corresponding to the rows and columns of the stft matrix \n",
    "# in \"s_scipy\"\n",
    "fs=sr \n",
    "noverlap = nperseg - hop_length\n",
    "\n",
    "freq_scipy, time_scipy, s_scipy = scipy.signal.stft(\n",
    "    audio, \n",
    "    fs=fs, window=\"hann\", nfft=nfft, \n",
    "    nperseg=nperseg, noverlap=noverlap\n",
    ")\n",
    "\n",
    "print(f\"freq vector shape: {freq_scipy.shape}\")\n",
    "print(f\"time vector shape: {time_scipy.shape}\")\n",
    "print(f\"stft matrix shape: {s_scipy.shape}\")\n",
    "\n",
    "s_scipy_db = librosa.amplitude_to_db(np.abs(s_scipy), ref=np.max)\n",
    "\n",
    "im = plt.imshow(s_scipy_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7dac42",
   "metadata": {},
   "source": [
    "## Recording from Microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "\n",
    "def record_audio(n_seconds: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Record audio using computer microphone.\n",
    "    \"\"\"\n",
    "    chunk = 1024\n",
    "    bit_depth = pyaudio.paInt16\n",
    "    n_channels = 1\n",
    "    sample_rate = 48000\n",
    "\n",
    "    input(\"Press Enter to begin recording ðŸŽ¤\")\n",
    "    print(\"ðŸŽ¤ Listening for music\", end=\"\\r\")\n",
    "\n",
    "    outfile = \"microphone_sample.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=bit_depth, channels=n_channels, rate=sample_rate, input=True, frames_per_buffer=chunk)\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(0, int(sample_rate / chunk * n_seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "\n",
    "    audio_np = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "    scipy.io.wavfile.write(outfile, sample_rate, audio_np)\n",
    "    print(f\"âœ… Recording saved to {outfile}\", end=\"\\n\")\n",
    "\n",
    "    return outfile\n",
    "\n",
    "#audio_path = record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799cf718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_path = \"microphone_sample.wav\"\n",
    "\n",
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "S_db = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "im = plt.imshow(S_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
