{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c9acb8",
   "metadata": {},
   "source": [
    "# Intro to Digital Audio\n",
    "\n",
    "To be able to work with audio signals in a computer, we need to create a series of discrete values from a continuous sound wave.\n",
    "\n",
    "This notebook will walk you through the fundamentals of how audio is represented in Python, how to create a custom dataset of music using yt-dlp/musicdl, and how to visualize the waveform of an audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92e900",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MichiganDataScienceTeam/F25-Shazam/blob/main/notebooks/week1_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574769f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib librosa scipy yt-dlp pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ffeaa",
   "metadata": {},
   "source": [
    "\n",
    "# 1/3: Some Terminology\n",
    "\n",
    "TODO: convert to paragraph form\n",
    "\n",
    "Explain Frequency, discretization of audio:\n",
    "\n",
    "1 Hertz = 1 vibration / second \n",
    "\n",
    "vibration of air particles\n",
    "\n",
    "![metadata](./asset/sample_metadata.png)\n",
    "\n",
    "Let's break down some common aspects of an audio file:\n",
    "\n",
    "- **sample rate** - number of samples per second (Hz). The sampling rate determines the time resolution of our representation.\n",
    "- **bit depth** - The number of bits used to store each sample. The larger the bit depth, the more precision we have to store frequency information (frequency resolution).\n",
    "\n",
    "\n",
    "time is digitized using the sample rate, amplitude is digitized using bit depth\n",
    "\n",
    "$\\mathrm{bit rate} = \\mathrm{sr}\\times\\mathrm{bit depth}\\times\\textrm{number of channels}$\n",
    "\n",
    "mp3 files are measured in bit rate since they do not store the associated bit depths\n",
    "\n",
    "The sampling interval is defined as the reciprocal of the sampling rate, and it represents the time duration between two consecutive samples in a discrete time signal. If the sampling rate is 44100 times per second, then the sampling interval is 1/44100.\n",
    "\n",
    "\n",
    "try different values of `sample_rate` and `freq_Hz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ce5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of the audio signal\n",
    "freq_Hz = 5\n",
    "\n",
    "# rate at which audio is being sampled\n",
    "sample_rate_Hz = 10\n",
    "\n",
    "x_continuous = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_continuous = np.cos(freq_Hz * 2*np.pi*x_continuous)\n",
    "\n",
    "x_discrete = np.linspace(0, 1, sample_rate_Hz, endpoint=False)\n",
    "y_discrete = np.cos(freq_Hz * 2*np.pi*x_discrete)\n",
    "\n",
    "plt.plot(x_continuous, y_continuous)\n",
    "plt.scatter(x_discrete, y_discrete, color=\"red\", s=50)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3c2bb",
   "metadata": {},
   "source": [
    "## Good to Know Sampling Theorem:\n",
    "\n",
    "If a signal contains no frequencies higher than $f_\\mathrm{max}$, then the signal can be perfectly reconstructed when sampled at a rate $sr > 2f_\\mathrm{max}$. In other words, the maximum reconstructable frequency is strictly less than $sr/2$. This is called the Nyquist-Shannon sampling theorem.\n",
    "\n",
    "Explain aliasing briefly\n",
    "\n",
    "Question: In order to cover the human hearing range of 20 Hz to 20 kHz, what is the minimum sampling rate required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_rate_required = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_Hz = 2\n",
    "\n",
    "freq_Hz = 5\n",
    "\n",
    "freq_reconstructable_up_to = sample_rate_Hz / 2\n",
    "\n",
    "x_continuous = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_continuous = np.cos(freq_Hz * 2*np.pi*x_continuous)\n",
    "\n",
    "x_undersampled = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_undersampled = np.cos(freq_reconstructable_up_to * 2*np.pi*x_undersampled)\n",
    "\n",
    "x_discrete = np.linspace(0, 1, sample_rate_Hz, endpoint=False)\n",
    "y_discrete = np.cos(freq_Hz * 2*np.pi*x_discrete)\n",
    "\n",
    "plt.plot(x_continuous, y_continuous)\n",
    "plt.plot(x_undersampled, y_undersampled, color=\"red\", linestyle=\"--\")\n",
    "plt.scatter(x_discrete, y_discrete, color=\"red\", s=50)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61deb37b",
   "metadata": {},
   "source": [
    "# 2/3: Obtaining audio files\n",
    "\n",
    "To create a database of songs that our Shazam clone will be able to recognize, we can use the [yt-dlp](https://github.com/yt-dlp/yt-dlp) Python package to download audio files directly from YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97125266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import os\n",
    "\n",
    "# add any youtube video url here, \n",
    "# copied from browser address bar\n",
    "youtube_url = \"\"\n",
    "\n",
    "yt_audio_path = \"yt_sample.wav\"\n",
    "\n",
    "ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': \"yt_sample.%(ext)s\",  # output file\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',     # save as wav file\n",
    "        }],\n",
    "        #'cookiefile': 'cookies.txt',\n",
    "    }\n",
    "if youtube_url != \"\":\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34d356",
   "metadata": {},
   "source": [
    "## musicdl: helper script for downloading audio\n",
    "\n",
    "For convienience, we've created a yt-dlp wrapper program that accepts urls from either YouTube or Spotify, and downloads the respective audio files to a specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/dennisfarmer/musicdl.git\n",
    "%pip install -e ./musicdl\n",
    "from musicdl.yt import YoutubeDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b074ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(YoutubeDownloader.download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "ydl = YoutubeDownloader(\n",
    "    audio_directory=\"./tracks\", \n",
    "    audio_format=\"wav\"\n",
    ")\n",
    "\n",
    "youtube_urls = [\n",
    "    \"https://www.youtube.com/watch?v=TqxfdNm4gZQ\"\n",
    "]\n",
    "\n",
    "tracks_info = ydl.download(youtube_urls)\n",
    "pprint(tracks_info)\n",
    "\n",
    "# write to csv\n",
    "tracks_csv = \"./tracks/tracks_info.csv\"\n",
    "tracks_df = pd.DataFrame(tracks_info)\n",
    "tracks_df.to_csv(tracks_csv, index=False)\n",
    "\n",
    "# read from csv\n",
    "#with open(tracks_csv, \"r\") as f:\n",
    "    #tracks_df = pd.read_csv(f)\n",
    "    #tracks_list = list(tracks_df.to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0884c",
   "metadata": {},
   "source": [
    "# 3/3: Working with audio data in Python:\n",
    "\n",
    "## Visualizing the amplitude and sample rate of an audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac27996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_path = yt_audio_path\n",
    "audio_path = \"sample.wav\"\n",
    "\n",
    "# default is to convert to mono (1 channel)\n",
    "audio, sr = librosa.load(audio_path, sr=None)  # sr=None uses the file's sampling rate\n",
    "print(f\"sample rate = {sr} Hz\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(audio, lw=0.1)  # interpolated waveform\n",
    "plt.scatter(np.arange(len(audio)), audio, s=0.005, color=\"red\")  # individual samples\n",
    "plt.title(\"Audio Waveform\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "print(\"audio is digitally represented as a numpy.ndarray:\")\n",
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e8a67",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "\n",
    "https://en.wikipedia.org/wiki/Downsampling_(signal_processing)\n",
    "\n",
    "internally, downsampling (reducing the sample rate) performs the following process:\n",
    "1. uses a low pass filter to remove higher frequency components to avoid aliasing (Sampling Theorem)\n",
    "    - low pass: keep signals with frequency lower than a specified cutoff frequency\n",
    "    - question: what would the cutoff frequency be if we resampled to 11,025 Hz?\n",
    "2. keeps every nth sample using a sample step size (\"decimation factor\") of $(\\mathrm{sr_{old}}/\\mathrm{sr_{new}})$, \n",
    "    - the details of this process are not super important to know\n",
    "    - if $(\\mathrm{sr_{old}}/\\mathrm{sr_{new}})$ is not an integer, first upsamples via interpolation, then downsamples with step size $\\mathrm{sr_{old}}$.\n",
    "    - upsampling: each original sample is separated by inserting $(\\mathrm{sr_{new}}-1)$ zeros, then signal is interpolated with filter to smooth out discontinuities (replacing zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample from 48,000 Hz to 11,025 Hz\n",
    "sr = 11_025\n",
    "audio, sr = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "# cutoff_frequency = ???\n",
    "\n",
    "# Question: how would you plot just the first 10 seconds of an audio file?\n",
    "#            Modify the existing code to do this\n",
    "# Hint: use sample rate\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(audio, lw=0.1)  # interpolated waveform\n",
    "plt.scatter(np.arange(len(audio)), audio, s=0.005, color=\"red\")  # individual samples\n",
    "plt.title(\"Resampled Audio Waveform\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653caec9",
   "metadata": {},
   "source": [
    "# Task for today:\n",
    "\n",
    "1) download the audio from any youtube video as a .wav file\n",
    "2) downsample the file to 44.1 kHz\n",
    "3) crop the audio to the first 10 seconds\n",
    "4) visualize the waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c37e75",
   "metadata": {},
   "source": [
    "---\n",
    "# Next week: visualizing frequency using a spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "audio_path = \"log_scale_perception.wav\"\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee19bbe",
   "metadata": {},
   "source": [
    "## Method 1: Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "\n",
    "# parameters of the short-time Fourier transform:\n",
    "# (algorithm that creates the spectrogram)\n",
    "win_length = 2**11  # number of samples in each window\n",
    "n_fft = win_length\n",
    "hop_length = win_length // 4\n",
    "window = scipy.signal.get_window(\"triang\", Nx=win_length)\n",
    "\n",
    "S = librosa.stft(audio, \n",
    "                       n_fft=n_fft, hop_length=hop_length, \n",
    "                       win_length=win_length, window=window)\n",
    "S_magnitude = np.abs(S)  # |a+bi| = sqrt(a^2 + b^2)\n",
    "S_db = librosa.amplitude_to_db(S_magnitude, ref=np.max)\n",
    "\n",
    "im = plt.imshow(S_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bfa7c4",
   "metadata": {},
   "source": [
    "## Method 2: Scipy (what we'll use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07492ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "\n",
    "# parameters of the short-time Fourier transform:\n",
    "# (algorithm that creates the spectrogram)\n",
    "nperseg = win_length = 2**11  # number of samples in each window\n",
    "nfft = n_fft = win_length\n",
    "hop_length = win_length // 4\n",
    "window = scipy.signal.get_window(\"triang\", Nx=win_length)\n",
    "\n",
    "# scipy.signal.stft also uses the sample rate to output \n",
    "# frequency (in Hz) and time (in seconds) vectors,\n",
    "# corresponding to the rows and columns of the stft matrix \n",
    "# in \"s_scipy\"\n",
    "fs=sr \n",
    "noverlap = nperseg - hop_length\n",
    "\n",
    "freq_scipy, time_scipy, s_scipy = scipy.signal.stft(\n",
    "    audio, \n",
    "    fs=fs, window=\"hann\", nfft=nfft, \n",
    "    nperseg=nperseg, noverlap=noverlap\n",
    ")\n",
    "\n",
    "print(f\"freq vector shape: {freq_scipy.shape}\")\n",
    "print(f\"time vector shape: {time_scipy.shape}\")\n",
    "print(f\"stft matrix shape: {s_scipy.shape}\")\n",
    "\n",
    "s_scipy_db = librosa.amplitude_to_db(np.abs(s_scipy), ref=np.max)\n",
    "\n",
    "im = plt.imshow(s_scipy_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7dac42",
   "metadata": {},
   "source": [
    "## Recording from Microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "\n",
    "def record_audio(n_seconds: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    records audio using computer microphone\n",
    "    \"\"\"\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 48000\n",
    "\n",
    "\n",
    "    input(\"Press Enter to begin recording ðŸŽ¤\")\n",
    "    print(\"ðŸŽ¤ Listening for music\", end=\"\\r\")\n",
    "\n",
    "    outfile = \"microphone_sample.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * n_seconds)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "\n",
    "    audio_np = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "    scipy.io.wavfile.write(outfile, RATE, audio_np)\n",
    "    print(f\"âœ… Recording saved to {outfile}\", end=\"\\n\")\n",
    "\n",
    "    return outfile\n",
    "\n",
    "#audio_path = record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799cf718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_path = \"microphone_sample.wav\"\n",
    "\n",
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "S_db = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "im = plt.imshow(S_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
